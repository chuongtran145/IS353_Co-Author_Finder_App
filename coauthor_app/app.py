import streamlit as st
import pandas as pd
import networkx as nx
import numpy as np
import math
from xgboost import XGBClassifier
from networkx.algorithms.community import louvain_communities, asyn_lpa_communities

# Cáº¥u hÃ¬nh trang
st.set_page_config(page_title="Co-Author Recommendation", layout="wide")

# ==========================================
# 1. HÃ€M Xá»¬ LÃ DATA & TRAINING (CORE LOGIC)
# ==========================================

@st.cache_resource
def train_pipeline(uploaded_file):
    """
    HÃ m nÃ y thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh:
    Load Data -> Build Graph -> Feature Eng -> Train XGBoost
    ÄÆ°á»£c cache láº¡i Ä‘á»ƒ khÃ´ng cháº¡y láº¡i má»—i láº§n reload trang.
    """
    with st.spinner('Äang xá»­ lÃ½ dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh... (CÃ³ thá»ƒ máº¥t vÃ i phÃºt)'):
        # 1.1 Load Data
        edges = []
        # Äá»c file upload (bytes -> string)
        content = uploaded_file.getvalue().decode("utf-8")
        for line in content.splitlines():
            if line.startswith("#"): continue
            parts = line.strip().split()
            if len(parts) < 2: continue
            u, v = int(parts[0]), int(parts[1])
            if u == v: continue
            if u > v: u, v = v, u
            edges.append((u, v))
        
        edges = list(set(edges)) # Remove duplicates
        
        # 1.2 Build Graph
        G = nx.Graph()
        G.add_edges_from(edges)
        
        # Core filtering (k=1 nhÆ° trong notebook)
        G_core = nx.k_core(G, k=1)
        
        # Split Train/Test (á»ž Ä‘Ã¢y ta dÃ¹ng toÃ n bá»™ G_core lÃ m G_train Ä‘á»ƒ demo cho Ä‘áº§y Ä‘á»§ dá»¯ liá»‡u)
        # Trong thá»±c táº¿ production, ta sáº½ train trÃªn toÃ n bá»™ dá»¯ liá»‡u hiá»‡n cÃ³
        G_train = G_core 
        
        # 1.3 Community Detection
        # Louvain
        louvain_comms = louvain_communities(G_train, seed=42)
        louvain_map = {}
        for idx, comm in enumerate(louvain_comms):
            for node in comm:
                louvain_map[node] = idx
                
        # Label Propagation
        lpa_comms = asyn_lpa_communities(G_train, seed=42)
        lpa_map = {}
        for idx, comm in enumerate(lpa_comms):
            for node in comm:
                lpa_map[node] = idx
        
        # 1.4 Generate Training Data (Simplified for Demo Speed)
        # Láº¥y máº«u 1 pháº§n Ä‘á»ƒ train cho nhanh (hoáº·c train full náº¿u server máº¡nh)
        # á»ž Ä‘Ã¢y mÃ¬nh tÃ¡i táº¡o logic feature extraction Ä‘á»ƒ train model
        
        # Sinh máº«u Positive
        train_edges = list(G_train.edges())
        # Giá»›i háº¡n sá»‘ lÆ°á»£ng máº«u train Ä‘á»ƒ demo cháº¡y nhanh (vÃ­ dá»¥ 10k máº«u pos)
        # Náº¿u muá»‘n chÃ­nh xÃ¡c nhÆ° notebook gá»‘c thÃ¬ bá» Ä‘oáº¡n slice [:10000]
        sample_pos = train_edges
        
        X = []
        y = []
        
        # HÃ m tÃ­nh feature cho 1 cáº·p (u, v)
        def compute_features(u, v, graph):
            # Basic sets
            neu = set(graph.neighbors(u))
            nev = set(graph.neighbors(v))
            common = neu.intersection(nev)
            union_set = neu.union(nev)
            
            # 1. Common Neighbors
            cn = len(common)
            
            # 2. Jaccard
            jaccard = cn / len(union_set) if len(union_set) > 0 else 0
            
            # 3. Adamic-Adar & 4. Resource Allocation
            aa = 0
            ra = 0
            for w in common:
                deg_w = graph.degree(w)
                if deg_w > 1:
                    aa += 1 / math.log(deg_w)
                if deg_w > 0:
                    ra += 1 / deg_w
            
            # 5. Preferential Attachment
            du = graph.degree(u)
            dv = graph.degree(v)
            pa = du * dv
            
            # 6. Community
            same_louvain = 1 if louvain_map.get(u, -1) == louvain_map.get(v, -2) else 0
            same_lpa = 1 if lpa_map.get(u, -1) == lpa_map.get(v, -2) else 0
            
            return [cn, aa, ra, jaccard, pa, du, dv, same_louvain, same_lpa]

        # Táº¡o dá»¯ liá»‡u Positive
        for u, v in sample_pos:
            feats = compute_features(u, v, G_train)
            X.append(feats)
            y.append(1)
            
        # Táº¡o dá»¯ liá»‡u Negative (Ratio 1:1 cho nhanh, notebook lÃ  1:5)
        num_neg = len(sample_pos)
        cnt = 0
        nodes_list = list(G_train.nodes())
        while cnt < num_neg:
            u_rnd = np.random.choice(nodes_list)
            v_rnd = np.random.choice(nodes_list)
            if u_rnd != v_rnd and not G_train.has_edge(u_rnd, v_rnd):
                feats = compute_features(u_rnd, v_rnd, G_train)
                X.append(feats)
                y.append(0)
                cnt += 1
                
        # 1.5 Train Model
        model = XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            eval_metric='logloss',
            random_state=42
        )
        model.fit(np.array(X), np.array(y))
        
        return model, G_train, louvain_map, lpa_map

# ==========================================
# 2. UI CHÃNH
# ==========================================

st.title("ðŸ”Ž Co-Author Finder System")
st.markdown("""
Há»‡ thá»‘ng gá»£i Ã½ Ä‘á»“ng tÃ¡c giáº£ dá»±a trÃªn **XGBoost** vÃ  **Graph Mining**.
Upload dataset (Ä‘á»‹nh dáº¡ng edge list `.txt`) Ä‘á»ƒ báº¯t Ä‘áº§u.
""")

# Sidebar: Upload Dataset
with st.sidebar:
    st.header("1. Dataset Selection")
    uploaded_file = st.file_uploader("Chá»n file dataset (VD: ca-HepPh.txt)", type=['txt'])
    
    st.info("Format: File text, má»—i dÃ²ng lÃ  `u v` hoáº·c `u \t v`. DÃ²ng báº¯t Ä‘áº§u báº±ng # sáº½ bá»‹ bá» qua.")

if uploaded_file is not None:
    # Trigger pipeline
    try:
        model, G_train, louvain_map, lpa_map = train_pipeline(uploaded_file)
        st.success(f"âœ… ÄÃ£ train xong mÃ´ hÃ¬nh! Sá»‘ lÆ°á»£ng Nodes: {G_train.number_of_nodes()}, Edges: {G_train.number_of_edges()}")
        
        # Main Area: Input Author ID
        st.header("2. Link Prediction / Recommendation")
        
        col1, col2 = st.columns([1, 2])
        with col1:
            # Chá»n 1 ID cÃ³ sáºµn Ä‘á»ƒ demo cho tiá»‡n
            example_id = list(G_train.nodes())[0]
            target_u = st.number_input("Nháº­p ID TÃ¡c giáº£ (Author ID):", min_value=0, value=example_id)
            top_k = st.slider("Sá»‘ lÆ°á»£ng gá»£i Ã½ (Top-k):", 5, 50, 10)
            btn_run = st.button("ðŸš€ Gá»£i Ã½ Äá»“ng tÃ¡c giáº£")

        if btn_run:
            if target_u not in G_train:
                st.error(f"Author ID {target_u} khÃ´ng tá»“n táº¡i trong Ä‘á»“ thá»‹!")
            else:
                with st.spinner(f"Äang tÃ¬m kiáº¿m á»©ng viÃªn 2-hop cho {target_u}..."):
                    # --- BÆ¯á»šC INFERENCE ---
                    
                    # 1. TÃ¬m á»©ng viÃªn 2-hop (Neighbors of Neighbors)
                    neighbors = set(G_train.neighbors(target_u))
                    candidates = set()
                    for n in neighbors:
                        candidates.update(G_train.neighbors(n))
                    
                    # Loáº¡i bá» chÃ­nh nÃ³ vÃ  cÃ¡c neighbor trá»±c tiáº¿p (Ä‘Ã£ lÃ  co-author rá»“i)
                    candidates.discard(target_u)
                    candidates = list(candidates - neighbors)
                    
                    if not candidates:
                        st.warning("KhÃ´ng tÃ¬m tháº¥y á»©ng viÃªn 2-hop nÃ o (Author nÃ y cÃ³ thá»ƒ bá»‹ cÃ´ láº­p hoáº·c Ä‘Ã£ káº¿t ná»‘i háº¿t).")
                    else:
                        # 2. TÃ­nh feature cho candidates
                        # Copy logic compute_features tá»« trÃªn xuá»‘ng Ä‘á»ƒ dÃ¹ng cho inference
                        X_pred = []
                        valid_candidates = []
                        
                        # Cache thÃ´ng tin node u Ä‘á»ƒ tÃ­nh nhanh
                        neu = neighbors # set
                        du = len(neu)
                        comm_u_louvain = louvain_map.get(target_u, -1)
                        comm_u_lpa = lpa_map.get(target_u, -1)
                        
                        candidate_details = [] # LÆ°u thÃ´ng tin giáº£i thÃ­ch
                        
                        for v in candidates:
                            nev = set(G_train.neighbors(v))
                            common = neu.intersection(nev) # Justification Path chÃ­nh lÃ  táº­p nÃ y
                            union_set = neu.union(nev)
                            
                            cn = len(common)
                            jaccard = cn / len(union_set) if len(union_set) > 0 else 0
                            
                            aa = 0
                            ra = 0
                            for w in common:
                                deg_w = G_train.degree(w)
                                if deg_w > 1: aa += 1 / math.log(deg_w)
                                if deg_w > 0: ra += 1 / deg_w
                            
                            dv = len(nev)
                            pa = du * dv
                            
                            sl = 1 if comm_u_louvain == louvain_map.get(v, -2) else 0
                            slpa = 1 if comm_u_lpa == lpa_map.get(v, -2) else 0
                            
                            feats = [cn, aa, ra, jaccard, pa, du, dv, sl, slpa]
                            X_pred.append(feats)
                            valid_candidates.append(v)
                            
                            # LÆ°u detail Ä‘á»ƒ hiá»ƒn thá»‹
                            candidate_details.append({
                                "id": v,
                                "common_neighbors": list(common),
                                "same_community": (sl == 1 or slpa == 1)
                            })
                        
                        # 3. Predict & Rank
                        if len(X_pred) > 0:
                            scores = model.predict_proba(np.array(X_pred))[:, 1] # Láº¥y xÃ¡c suáº¥t lá»›p 1
                            
                            # GhÃ©p káº¿t quáº£
                            results = []
                            for i, v in enumerate(valid_candidates):
                                results.append({
                                    "Candidate ID": v,
                                    "Score (Probability)": float(scores[i]),
                                    "Common Neighbors Count": len(candidate_details[i]["common_neighbors"]),
                                    "Common Neighbors List": candidate_details[i]["common_neighbors"],
                                    "Same Community": candidate_details[i]["same_community"]
                                })
                            
                            # Sort desc
                            df_res = pd.DataFrame(results).sort_values(by="Score (Probability)", ascending=False).head(top_k)
                            
                            # --- HIá»‚N THá»Š Káº¾T QUáº¢ ---
                            st.subheader(f"Top {top_k} Gá»£i Ã½ cho Author {target_u}")
                            
                            # Báº£ng tá»•ng quan
                            st.dataframe(
                                df_res[["Candidate ID", "Score (Probability)", "Common Neighbors Count", "Same Community"]],
                                use_container_width=True
                            )
                            
                            # Justification Paths (Chi tiáº¿t)
                            st.markdown("### ðŸ›¤ï¸ Justification & Paths")
                            for idx, row in df_res.iterrows():
                                c_id = row["Candidate ID"]
                                score = row["Score (Probability)"]
                                cn_list = row["Common Neighbors List"]
                                
                                with st.expander(f"ðŸ… Rank {idx+1}: Author **{c_id}** (Score: {score:.4f})"):
                                    col_a, col_b = st.columns(2)
                                    with col_a:
                                        st.write("**Why?**")
                                        st.write(f"- CÃ³ **{len(cn_list)}** báº¡n chung.")
                                        st.write(f"- CÃ¹ng cá»™ng Ä‘á»“ng: **{'Yes' if row['Same Community'] else 'No'}**")
                                    with col_b:
                                        st.write("**Justification Path (via):**")
                                        st.write(f"{target_u} â†” {cn_list[:10]}... â†” {c_id}")
                                        if len(cn_list) > 10:
                                            st.caption(f"*Hiá»ƒn thá»‹ 10/{len(cn_list)} nodes trung gian*")
                        else:
                            st.warning("KhÃ´ng tÃ­nh Ä‘Æ°á»£c feature cho candidates.")

    except Exception as e:
        st.error(f"CÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ file: {e}")
        st.write("Vui lÃ²ng Ä‘áº£m báº£o file Ä‘Ãºng Ä‘á»‹nh dáº¡ng edge list.")

else:
    st.info("ðŸ‘ˆ Vui lÃ²ng upload file dataset tá»« thanh bÃªn trÃ¡i.")